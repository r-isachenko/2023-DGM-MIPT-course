\input{../utils/preamble}
\createdgmtitle{14}

\usepackage{tikz}

\usetikzlibrary{arrows,shapes,positioning,shadows,trees}
%--------------------------------------------------------------------------------
\begin{document}
%--------------------------------------------------------------------------------
\begin{frame}[noframenumbering,plain]
%\thispagestyle{empty}
\titlepage
\end{frame}
%=======
\begin{frame}{Recap of previous lecture}
	\begin{figure}
		\centering
		\includegraphics[width=0.75\linewidth]{figs/smld}
	\end{figure}
	\vspace{-0.3cm} 
	\begin{block}{Theorem (implicit score matching)}
		\vspace{-0.6cm}
		\[
		{\color{violet}\frac{1}{2} \bbE_{\pi}\bigl\| \bs_{\btheta}(\bx) - \nabla_\bx \log \pi(\bx) \bigr\|^2_2} = {\color{teal}\bbE_{\pi}\Bigr[ \frac{1}{2}\| \bs_{\btheta}(\bx) \|_2^2 + \text{tr}\bigl(\nabla_{\bx} \bs_{\btheta}(\bx)\bigr) \Bigr]} + \text{const}
		\]
	\end{block}
	\vspace{-0.5cm}
	\begin{enumerate}
		\item {\color{violet}The left hand side} is intractable due to unknown $\pi(\bx)$ -- \textbf{denoising score matching}. 
		\item {\color{teal}The right hand side} is complex due to Hessian matrix -- \textbf{sliced score matching (Hutchinson's trace estimation)}.
	\end{enumerate}
	\myfootnotewithlink{https://yang-song.github.io/blog/2021/score/}{Song Y. Generative Modeling by Estimating Gradients of the Data Distribution, blog post, 2021}
\end{frame}
%=======
\begin{frame}{Recap of previous lecture}
	Let perturb original data by normal noise $p(\bx' | \bx, \sigma) = \cN(\bx' | \bx, \sigma^2 \bI)$
	\vspace{-0.3cm}
	\[
	\pi(\bx' | \sigma) = \int \pi(\bx) p(\bx' | \bx, \sigma) d\bx.
	\]
	\vspace{-0.6cm} \\
	Then the solution of 
	\vspace{-0.2cm}
	\[
	\frac{1}{2} \bbE_{\pi(\bx' | \sigma)}\bigl\| \bs_{\btheta}(\bx', \sigma) - \nabla_{\bx'} \log \pi(\bx' | \sigma) \bigr\|^2_2 \rightarrow \min_{\btheta}
	\]
	\vspace{-0.5cm} \\
	satisfies $\bs_{\btheta}(\bx', \sigma) \approx \bs(\bx', \btheta, 0) = \bs(\bx', \btheta)$ if $\sigma$ is small enough.
	\begin{block}{Theorem (denoising score matching)}
		\vspace{-0.8cm}
		\begin{multline*}
			\bbE_{\pi(\bx' | \sigma)}\bigl\| \bs_{\btheta}(\bx', \sigma) - \nabla_{\bx'} \log \pi(\bx' | \sigma) \bigr\|^2_2 = \\ = \bbE_{\pi(\bx)} \bbE_{p(\bx' | \bx, \sigma)}\bigl\| \bs_{\btheta}(\bx', \sigma) - \nabla_{\bx'} \log p(\bx' | \bx, \sigma) \bigr\|^2_2 + \text{const}(\btheta)
		\end{multline*}
		\vspace{-0.8cm}
	\end{block}
	Here $\nabla_{\bx'} \log p(\bx' | \bx, \sigma) = - \frac{\bx' - \bx}{\sigma^2}$.
	\begin{itemize}
		\item The RHS does not need to compute $\nabla_{\bx'} \log \pi(\bx' | \sigma)$ and even more $\nabla_{\bx'} \log \pi(\bx')$.
		\item $\bs_{\btheta}(\bx', \sigma)$ tries to \textbf{denoise} a corrupted sample.
		\item Score function $\bs_{\btheta}(\bx', \sigma)$ parametrized by $\sigma$. 
	\end{itemize}
	\myfootnotewithlink{http://www.iro.umontreal.ca/~vincentp/Publications/smdae_techreport.pdf}{Vincent P. A Connection Between Score Matching and Denoising Autoencoders, 2010}
\end{frame}
%=======
\begin{frame}{Recap of previous lecture}
	\begin{block}{Noise conditioned score network}
		\begin{itemize}
			\item Define the sequence of noise levels: $\sigma_1 > \sigma_2 > \dots > \sigma_L$.
			\item Train denoised score function $\bs_{\btheta}(\bx', \sigma)$ for each noise level:
			\vspace{-0.3cm}
			\[
				\sum_{l=1}^L {\color{violet}\sigma_l^2} \bbE_{\pi(\bx)} \bbE_{p(\bx' | \bx, \sigma_l)}\bigl\| \bs_{\btheta}(\bx', \sigma_l) - \nabla_\bx' \log p(\bx' | \bx, \sigma_l) \bigr\|^2_2 \rightarrow \min_{\btheta}
			\]
			\vspace{-0.5cm}
			\item Sample from \textbf{annealed} Langevin dynamics (for $l=1, \dots, L$).
		\end{itemize}
	\end{block}
	\begin{figure}
		\includegraphics[width=0.55\linewidth]{figs/multi_scale}
	\end{figure}
	\begin{figure}
		\includegraphics[width=\linewidth]{figs/duoduo}
	\end{figure}
	\myfootnotewithlink{https://arxiv.org/abs/1907.05600}{Song Y. et al. Generative Modeling by Estimating Gradients of the Data Distribution, 2019}
\end{frame}
%=======
\begin{frame}{Recap of previous lecture}
	\begin{block}{NCSN training}
		\begin{enumerate}
			\item Get the sample $\bx_0 \sim \pi(\bx)$.
			\item Sample noise level $l \sim U[1, L]$ and the noise $\bepsilon \sim \cN(0, \bI)$.
			\item Get noisy image $\bx' = \bx_0 + \sigma_l \cdot \bepsilon$.
			\item Compute loss $ \cL = \| \bs_{\btheta}(\bx', \sigma_l) + \frac{\bepsilon}{\sigma_l} \|^2 $.
		\end{enumerate}
	\end{block}
	\begin{block}{NCSN sampling (annealed Langevin dynamics)}
		\begin{itemize}
			\item Sample $\bx_0 \sim \cN(0, \sigma_1 \bI) \approx \pi(\bx | \sigma_L)$.
			\item Apply $T$ steps of Langevin dynamic
			\vspace{-0.2cm}
			\[
				\bx_t = \bx_{t-1} + \frac{1}{2}\eta_l \bs_{\btheta}(\bx_{t - 1}, \sigma_l) + \sqrt{\eta_l} \bepsilon_t.
			\] 
			\vspace{-0.5cm}
			\item Update $\bx_0 := \bx_T$ and choose the next $\sigma_l$.
		\end{itemize}
	\end{block}
	\myfootnotewithlink{https://arxiv.org/abs/2006.09011}{Song Y. et al. Improved Techniques for Training Score-Based Generative Models, 2020}
\end{frame}
%=======
\begin{frame}{Recap of previous lecture}
	\begin{block}{NCSN objective}
		\vspace{-0.2cm}
		\[
			{\color{olive}\bbE_{p(\bx' | \bx, \sigma_l)}}\bigl\| {\color{teal}\bs_{\btheta}(\bx', \sigma_l)} - {\color{violet}\nabla_{\bx'} \log p(\bx' | \bx, \sigma_l)} \bigr\|^2_2 \rightarrow \min_{\btheta}
		\]
	\end{block}
	\vspace{-0.3cm}
	\begin{block}{DDPM objective}
		\vspace{-0.2cm}
		\[
			\cL_t = {\color{olive}\bbE_{\bepsilon \sim \cN(0, \bI)}} \left[ \frac{(1 - \alpha_t)^2}{2\tilde{\beta}_t \alpha_t} \left\| {\color{violet}\frac{\bepsilon}{\sqrt{1 - \bar{\alpha}_t}}} - {\color{teal}\frac{\bepsilon_{\btheta}(\bx_t, t)}{\sqrt{1 - \bar{\alpha}_t}}}\right\|^2 \right]
		\]
		\vspace{-0.7cm}
	\end{block}
	\begin{align*}
		q(\bx_t | \bx_0) &= \cN(\bx_t | \sqrt{\bar{\alpha}_t} \cdot \bx_0, (1 - \bar{\alpha}_t) \cdot \bI) \\
		{\color{violet}\nabla_{\bx_t} \log q(\bx_t | \bx_0)} &= - \frac{\bx_t - \sqrt{\bar{\alpha}_t} \cdot \bx_0}{1 - \bar{\alpha}_t} = - \frac{\bepsilon}{\sqrt{1 - \bar{\alpha}_t}}.
	\end{align*}
		Let reparametrize our model: 
		\vspace{-0.2cm}
		\[
			{\color{teal}\bs_{\btheta}(\bx_t, t)} = - \frac{\bepsilon_{\btheta}(\bx_t, t)}{\sqrt{1 - \bar{\alpha}_t}}.
		\]
	\myfootnotewithlink{https://arxiv.org/abs/2006.11239}{Ho J. Denoising Diffusion Probabilistic Models, 2020}
	\end{frame}
%=======
\begin{frame}{Outline}
	\tableofcontents
\end{frame}
%=======
\section{NCSN DDPM SDE}
%=======
\begin{frame}{NCSN DDPM SDE}
	\[
		\bx_l = \bx_{l - 1} + \sqrt{\sigma^2_l - \sigma^2_{l-1}} \bepsilon
	\]
	\[
		\bx(t + dt) = \bx(t) + \sqrt{\frac{\sigma^2(t + dt) - \sigma^2(t)}{dt} dt} \cdot \bepsilon = \bx(t) + \sqrt{\frac{ d [\sigma^2(t)]}{dt}} \cdot d \bw
	\]
	\[
		d \bx = \sqrt{\frac{ d [\sigma^2(t)]}{dt}} \cdot d \bw
	\]
	\[
		\bx_t = \sqrt{1 - \beta_t} \cdot \bx_{t - 1} + \sqrt{\beta_t} \cdot \bepsilon
	\]
	\begin{multline*}
		\bx(t) = \sqrt{1 - \beta(t) dt} \cdot \bx(t - dt) + \sqrt{\beta(t)dt} \cdot \bepsilon = \\
		= (1 - \frac{1}{2} \beta(t) dt) \cdot \bx(t - dt) + \sqrt{\beta(t)dt} \cdot \bepsilon = \\
		= \bx(t - dt) - \frac{1}{2} \beta(t) \bx(t - dt) dt  + \sqrt{\beta(t)} \cdot d \bw
	\end{multline*}
	\[
		d \bx = - \frac{1}{2} \beta(t) \bx(t) dt + \sqrt{\beta(t)} \cdot d \bw
	\]
	\myfootnotewithlink{https://arxiv.org/abs/2105.05233}{Song Y., et al. Score-Based Generative Modeling through Stochastic Differential Equations, 2020}
\end{frame}
%=======
\section{Classifier guidance}
%=======
\begin{frame}{Classifier guidance}
	Idea: use $p(\bx_{t-1} | \bx_t, \by, \btheta)$ instead of $p(\bx_{t-1} | \bx_t, \btheta)$.
	\myfootnotewithlink{https://arxiv.org/abs/2105.05233}{Dhariwal P., Nichol A. Diffusion Models Beat GANs on Image Synthesis, 2021}
\end{frame}
%=======
\begin{frame}{Classifier guidance}
	\begin{itemize}
		\item Let imagine we are given the distribution $q(\by | \bx_0)$. 
		\item Since we have already defined Markov chain, we have $q(\bx_t | \bx_{t-1}, \by) = q(\bx_t | \bx_{t - 1})$.
		\item Let try to find reverse $q(\bx_{t-1} | \bx_t, \by)$.
		\item Helper statement:
		\vspace{-0.2cm}
		\begin{multline*}
			{\color{teal}q(\by | \bx_{t - 1}, \bx_t)} = \frac{q(\bx_{t - 1}, \bx_t, \by)}{q(\bx_{t - 1}, \bx_t)} = \\
			= \frac{q(\bx_t | \bx_{t - 1}, {\color{olive}\by}) q(\by | \bx_{t - 1}) \color{violet}{q(\bx_{t - 1})}} {q(\bx_t | \bx_{t - 1}) \color{violet}{q(\bx_{t - 1})}} = q(\by | \bx_{t - 1}).
		\end{multline*}
		\vspace{-0.4cm}
	\end{itemize}
	\begin{block}{Conditional distribution}
		\vspace{-0.7cm}
		\begin{multline*}
			q(\bx_{t - 1} | \bx_t, \by) = \frac{q(\bx_{t - 1}, \bx_t, \by)}{q(\bx_t, \by)}  = \\ 
			=  \frac{{\color{teal}q(\by | \bx_{t - 1}, \bx_t)} q(\bx_{t - 1} | \bx_t) {\color{violet}q(\bx_t)}}{q(\by | \bx_t) {\color{violet}q(\bx_t)}} = \\
			= q(\by | \bx_{t - 1}) q(\bx_{t - 1} | \bx_t) \cdot \text{const}(\bx_{t - 1}).
		\end{multline*}
	\end{block}
	\myfootnotewithlink{https://arxiv.org/abs/2105.05233}{Dhariwal P., Nichol A. Diffusion Models Beat GANs on Image Synthesis, 2021}
\end{frame}
%=======
\begin{frame}{Classifier guidance}
	\begin{block}{Conditional distribution}
		\[
			q(\bx_{t - 1} | \bx_t, \by) = q(\by | \bx_{t - 1}) q(\bx_{t - 1} | \bx_t) \cdot \text{const}(\bx_{t - 1}).
		\]
	\end{block}
	\vspace{-0.7cm}
	\[
		p(\bx_{t - 1} | \bx_t, \by, \btheta , \bphi) = p(\by | \bx_{t-1}, \bphi) p(\bx_{t-1} | \bx_t, \btheta) \cdot \text{const}(\bx_{t - 1}).
	\]
	\[
		p(\bx_{t-1} | \bx_t, \btheta) = \cN(\bmu_{\btheta}(\bx_t, t), \bsigma^2_{\btheta}(\bx_t, t)). 
	\]
	\[
		\log p(\bx_{t-1} | \bx_t, \btheta) = - \frac{\|\bx_{t - 1} - \bmu\|^2}{2 \bsigma^2} + \text{const}(\bx_{t-1})
	\]
	\begin{multline*}
		\log p(\by | \bx_{t-1}, \bphi) \approx \log p(\by | \bx_{t-1}, \bphi) |_{\bx_{t-1} = \bmu} + \\
		+ (\bx_{t-1} - \bmu) \nabla_{\bx_{t-1}} \log p(\by | \bx_{t-1}, \bphi) |_{\bx_{t-1} = \bmu} = \\
		= (\bx_{t-1} - \bmu) \cdot \mathbf{g} +  \text{const}(\bx_{t-1}),
	\end{multline*}
	where $\mathbf{g} =  \nabla_{\bx_{t-1}} \log p(\by | \bx_{t-1}, \bphi) |_{\bx_{t-1} = \bmu}$.
	\myfootnotewithlink{https://arxiv.org/abs/2105.05233}{Dhariwal P., Nichol A. Diffusion Models Beat GANs on Image Synthesis, 2021}
\end{frame}
%=======
\begin{frame}{Classifier guidance}
	\[
		\log p(\bx_{t - 1} | \bx_t, \by, \btheta , \bphi) = \log p(\by | \bx_{t-1}, \bphi) + \log p(\bx_{t-1} | \bx_t, \btheta) + \text{const}(\bx_{t - 1})
	\]
	\begin{align*}
		\log p(\bx_{t - 1} | \bx_t, \by, \btheta , \bphi) &= (\bx_{t-1} - \bmu) \cdot \mathbf{g} - \frac{\|\bx_{t - 1} - \bmu\|^2}{2 \bsigma^2} + \text{const}(\bx_{t-1}) \\
		& = - \frac{\|\bx_{t - 1} - \bmu - \bsigma \odot \mathbf{g}\|^2}{2 \bsigma^2} + \text{const}(\bx_{t-1}) \\
		& = \log \cN( \bmu + \bsigma \odot \mathbf{g}, \bsigma^2) + \text{const}(\bx_{t-1})
	\end{align*}
	\myfootnotewithlink{https://arxiv.org/abs/2105.05233}{Dhariwal P., Nichol A. Diffusion Models Beat GANs on Image Synthesis, 2021}
\end{frame}
%=======
\begin{frame}{Classifier guidance}
	\begin{block}{Guided sampling}
		\begin{enumerate}
			\item Sample $\bx_T \sim \cN(0, \bI)$.
			\item Compute mean of $p(\bx_{t-1} | \bx_t, \btheta) = \cN(\bmu_{\btheta}(\bx_t, t), \tilde{\beta}_t \bI)$:
			\[
				\bmu_{\btheta}(\bx_t, t) = \frac{1}{\sqrt{\alpha_t}} \bx_t - \frac{1 - \alpha_t}{\sqrt{\alpha_t (1 - \bar{\alpha}_t)}} \bepsilon_{\btheta}(\bx_t, t)
			\]
			\vspace{-0.3cm}
			\item Compute $\mathbf{g} =  \nabla_{\bx_{t-1}} \log p(\by | \bx_{t-1}, \bphi) |_{\bx_{t-1} = \bmu}$
			\item Get denoised image $\bx_{t - 1} = (\bmu_{\btheta}(\bx_t, t) + \tilde{\beta}_t \cdot \mathbf{g})+ \sqrt{\tilde{\beta}_t} \cdot \bepsilon$, where $\bepsilon \sim \cN(0, \bI)$.
		\end{enumerate}
	\end{block}
	\myfootnotewithlink{https://arxiv.org/abs/2105.05233}{Dhariwal P., Nichol A. Diffusion Models Beat GANs on Image Synthesis, 2021}
\end{frame}
%=======
\begin{frame}{Classifier guidance}
	\[
		\nabla_{\bx_t} \log p(\bx_t | \btheta) \approx - \frac{\bepsilon_{\btheta}(\bx_t, t)}{\sqrt{1 - \bar{\alpha}_t}}.
	\]
	\begin{align*}
		\nabla_{\bx_t} \log p(\bx_t | \by, \btheta, \bphi) &= \nabla_{\bx_t} \log p(\by | \bx_t, \bphi) + \nabla_{\bx_t} \log p(\bx_t | \btheta) \\
		&= \nabla_{\bx_t} \log p(\by | \bx_t, \bphi) - \frac{\bepsilon_{\btheta}(\bx_t, t)}{\sqrt{1 - \bar{\alpha}_t}}
	\end{align*}
	\[
		\hat{\bepsilon}_{\btheta}(\bx_t, t) = \bepsilon_{\btheta}(\bx_t, t) - \sqrt{1 - \bar{\alpha}_t} \cdot \nabla_{\bx_t} \log p(\by | \bx_t, \bphi)
	\]
	\[
		\hat{\bepsilon}_{\btheta}(\bx_t, t) = \bepsilon_{\btheta}(\bx_t, t) - \gamma \cdot \sqrt{1 - \bar{\alpha}_t} \cdot \nabla_{\bx_t} \log p(\by | \bx_t, \bphi)
	\]
	\[
		\nabla_{\bx_t}^{\gamma} \log p(\bx_t | \by, \btheta, \bphi) = \gamma \cdot \nabla_{\bx_t} \log p(\by | \bx_t, \bphi) + \nabla_{\bx_t} \log p(\bx_t | \btheta) 
	\]
	\[
		p(\bx_t | \by, \btheta, \bphi) = \frac{p(\by | \bx_t, \bphi)^{\gamma} p(\bx_t | \btheta)}{Z}
	\]
	Check that $\nabla_{\bx_t} Z \neq 0$.
	\myfootnotewithlink{https://arxiv.org/abs/2105.05233}{Dhariwal P., Nichol A. Diffusion Models Beat GANs on Image Synthesis, 2021}
\end{frame}
%=======
\begin{frame}{Classifier guidance}
	\begin{block}{Guided sampling}
		\begin{enumerate}
			\item Sample $\bx_T \sim \cN(0, \bI)$.
			\item Compute "corrected" $\hat{\bepsilon}_{\btheta}(\bx_t, t)$:
			\[
				\hat{\bepsilon}_{\btheta}(\bx_t, t) = \bepsilon_{\btheta}(\bx_t, t) - \gamma \cdot \sqrt{1 - \bar{\alpha}_t} \cdot \nabla_{\bx_t} \log p(\by | \bx_t, \bphi)
			\]
			\item Compute mean of $p(\bx_{t-1} | \bx_t, \btheta) = \cN(\bmu_{\btheta}(\bx_t, t), \tilde{\beta}_t \bI)$:
			\[
				\bmu_{\btheta}(\bx_t, t) = \frac{1}{\sqrt{\alpha_t}} \bx_t - \frac{1 - \alpha_t}{\sqrt{\alpha_t (1 - \bar{\alpha}_t)}} \hat{\bepsilon}_{\btheta}(\bx_t, t)
			\]
			\vspace{-0.3cm}
			\item Get denoised image $\bx_{t - 1} = \bmu_{\btheta}(\bx_t, t) +  \sqrt{\tilde{\beta}_t} \cdot \bepsilon$, where $\bepsilon \sim \cN(0, \bI)$.
		\end{enumerate}
	\end{block}
	\myfootnotewithlink{https://arxiv.org/abs/2105.05233}{Dhariwal P., Nichol A. Diffusion Models Beat GANs on Image Synthesis, 2021}
\end{frame}
%=======
\section{Classifier-free guidance}
%=======
\begin{frame}{Classifier-free guidance}
	\begin{align*}
		\nabla_{\bx_t}^{\gamma} \log p(\bx_t | \by, \btheta, \bphi) &= \gamma \nabla_{\bx_t} \log p(\by | \bx_t, \bphi) + \nabla_{\bx_t} \log p(\bx_t | \btheta) \\
		&= \gamma \nabla_{\bx_t} \log \left(\frac{p(\bx_t| \by, \btheta) p(\by | \bphi)}{p(\bx_t | \btheta)}\right) + \nabla_{\bx_t} \log p(\bx_t | \btheta)\\
				&= \gamma \nabla_{\bx_t} \log p(\bx_t| \by, \btheta) + (1 - \gamma) \nabla_{\bx_t} \log p(\bx_t | \btheta)
	\end{align*}
	\[
		\hat{\bepsilon}_{\btheta}(\bx_t, t) = \gamma \cdot \bepsilon_{\btheta}(\bx_t, \by, t) + (1 - \gamma) \cdot \bepsilon_{\btheta}(\bx_t, t)
	\]
	\myfootnotewithlink{https://arxiv.org/pdf/2112.10741}{Nichol A., et al. GLIDE: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models, 2022}
\end{frame}
%=======
\begin{frame}{Classifier-free guidance}
	â€œa stained glass window of a panda eating bamboo"
	\begin{figure}
		\includegraphics[width=\linewidth]{figs/cfg}
	\end{figure}
	\myfootnotewithlink{https://arxiv.org/pdf/2112.10741}{Nichol A., et al. GLIDE: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models, 2022}
\end{frame}
%=======
\section{The worst course overview}
%=======
\begin{frame}{The worst course overview :)}
	\begin{figure}
		\includegraphics[width=\linewidth]{figs/generative-overview}
	\end{figure}
	\myfootnotewithlink{https://lilianweng.github.io/posts/2021-07-11-diffusion-models/}{Weng L. What are Diffusion Models?, blog post, 2021}
\end{frame}
%=======
\begin{frame}{The worst course overview :)}
	\begin{figure}
		\includegraphics[width=0.7\linewidth]{figs/trilemma}
	\end{figure}
	\myfootnotewithlink{https://arxiv.org/abs/2112.07804}{Xiao Z., Kreis K., Vahdat A. Tackling the generative learning trilemma with denoising diffusion GANs, 2021}
\end{frame}
%=======
\begin{frame}{Summary}
	\begin{itemize}
		\item 
	\end{itemize}
\end{frame}
\end{document} 